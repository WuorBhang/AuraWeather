"""
Weather AI Model Training Script
Trains a deep learning model on real weather images for classification.
Based on the provided training code structure with EfficientNetB0.
"""

import os
import warnings
# Suppress TensorFlow warnings
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
warnings.filterwarnings('ignore')

import numpy as np
import tensorflow as tf
from tensorflow import keras
from keras import layers
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
from pathlib import Path

# Set matplotlib backend to avoid display issues
plt.switch_backend('Agg')

# Configure TensorFlow to use CPU only if GPU issues persist
try:
    # Try to import EfficientNetB0
    from keras.applications import EfficientNetB0
    EFFICIENTNET_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è  EfficientNetB0 not available, will use fallback architecture")
    EFFICIENTNET_AVAILABLE = False

# Force CPU usage if GPU causes issues
try:
    tf.config.set_visible_devices([], 'GPU')
    print("üîß Configured TensorFlow to use CPU only")
except:
    pass

# Configuration
IMG_SIZE = 224
BATCH_SIZE = 32
EPOCHS_INITIAL = 10  # Reduced for faster testing
EPOCHS_FINE_TUNE = 5   # Reduced for faster testing
NUM_CLASSES = 4
WEATHER_CLASSES = ['Cloudy', 'Rain', 'Sunrise', 'Shine']

# Data paths
TRAIN_DIR = 'weather_images/train_dir'
TEST_DIR = 'weather_images/test_dir'
MODEL_SAVE_PATH = 'weather_model.h5'
TFLITE_SAVE_PATH = 'assets/models/weather_model.tflite'

def check_data_structure():
    """Verify the data directory structure and count images"""
    print("Checking data structure...")
    
    if not os.path.exists(TRAIN_DIR):
        print(f"‚ùå Training directory not found: {TRAIN_DIR}")
        return False
    
    if not os.path.exists(TEST_DIR):
        print(f"‚ùå Test directory not found: {TEST_DIR}")
        return False
    
    train_counts = {}
    test_counts = {}
    
    for weather_class in WEATHER_CLASSES:
        train_path = os.path.join(TRAIN_DIR, weather_class)
        test_path = os.path.join(TEST_DIR, weather_class)
        
        if os.path.exists(train_path):
            train_counts[weather_class] = len([f for f in os.listdir(train_path) 
                                            if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))])
        else:
            train_counts[weather_class] = 0
            print(f"‚ö†Ô∏è  Training folder missing: {train_path}")
        
        if os.path.exists(test_path):
            test_counts[weather_class] = len([f for f in os.listdir(test_path) 
                                           if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))])
        else:
            test_counts[weather_class] = 0
            print(f"‚ö†Ô∏è  Test folder missing: {test_path}")
    
    print("\nüìä Dataset Summary:")
    print("=" * 50)
    total_train = 0
    total_test = 0
    
    for weather_class in WEATHER_CLASSES:
        train_count = train_counts[weather_class]
        test_count = test_counts[weather_class]
        total_train += train_count
        total_test += test_count
        
        print(f"{weather_class:12}: {train_count:4} train | {test_count:4} test")
    
    print("-" * 50)
    print(f"{'Total':12}: {total_train:4} train | {total_test:4} test")
    
    if total_train == 0:
        print("\n‚ùå No training images found! Please add images to the weather_images/train_dir folders.")
        return False
    
    if total_test == 0:
        print("\n‚ö†Ô∏è  No test images found. Using train/validation split instead.")
    
    return True

def load_and_preprocess_data():
    """Load and preprocess the weather image data"""
    print("\nüîÑ Loading and preprocessing data...")
    
    # Data augmentation for training
    train_datagen = keras.preprocessing.image.ImageDataGenerator(
        rescale=1./255,
        rotation_range=15,
        width_shift_range=0.1,
        height_shift_range=0.1,
        horizontal_flip=True,
        vertical_flip=False,
        zoom_range=0.1,
        brightness_range=[0.8, 1.2],
        validation_split=0.2  # Use 20% for validation if no test set
    )
    
    test_datagen = keras.preprocessing.image.ImageDataGenerator(rescale=1./255)
    
    # Load training data
    train_generator = train_datagen.flow_from_directory(
        TRAIN_DIR,
        target_size=(IMG_SIZE, IMG_SIZE),
        batch_size=BATCH_SIZE,
        class_mode='categorical',
        subset='training',
        shuffle=True,
        seed=42
    )
    
    # Load validation data
    validation_generator = train_datagen.flow_from_directory(
        TRAIN_DIR,
        target_size=(IMG_SIZE, IMG_SIZE),
        batch_size=BATCH_SIZE,
        class_mode='categorical',
        subset='validation',
        shuffle=True,
        seed=42
    )
    
    # Load test data if available
    test_generator = None
    if os.path.exists(TEST_DIR) and any(os.listdir(os.path.join(TEST_DIR, cls)) for cls in WEATHER_CLASSES):
        test_generator = test_datagen.flow_from_directory(
            TEST_DIR,
            target_size=(IMG_SIZE, IMG_SIZE),
            batch_size=BATCH_SIZE,
            class_mode='categorical',
            shuffle=False
        )
    
    print(f"‚úÖ Training samples: {train_generator.samples}")
    print(f"‚úÖ Validation samples: {validation_generator.samples}")
    if test_generator:
        print(f"‚úÖ Test samples: {test_generator.samples}")
    
    print(f"‚úÖ Class indices: {train_generator.class_indices}")
    
    return train_generator, validation_generator, test_generator

def visualize_data_samples(train_generator):
    """Display sample images from each class"""
    print("\nüì∏ Visualizing sample data...")
    
    fig, axes = plt.subplots(2, 4, figsize=(15, 8))
    fig.suptitle('Sample Weather Images', fontsize=16)
    
    class_names = list(train_generator.class_indices.keys())
    
    # Get samples for each class
    for i, class_name in enumerate(class_names):
        # Find the first image of this class
        found = False
        for batch_x, batch_y in train_generator:
            for j in range(len(batch_y)):
                if np.argmax(batch_y[j]) == i and not found:
                    # Display original
                    axes[0, i].imshow(batch_x[j])
                    axes[0, i].set_title(f'{class_name} (Original)')
                    axes[0, i].axis('off')
                    
                    # Display augmented version
                    if j + 1 < len(batch_x):
                        axes[1, i].imshow(batch_x[j + 1])
                        axes[1, i].set_title(f'{class_name} (Augmented)')
                        axes[1, i].axis('off')
                    
                    found = True
                    break
            if found:
                break
    
    plt.tight_layout()
    plt.savefig('data_samples.png', dpi=150, bbox_inches='tight')
    print("‚úÖ Sample visualization saved as 'data_samples.png'")
    plt.close()

def build_weather_model():
    """Build the weather classification model using EfficientNetB0 or fallback"""
    print("\nüèóÔ∏è  Building model architecture...")
    
    try:
        # Input layer
        inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))
        
        # Try to load EfficientNetB0 if available
        if EFFICIENTNET_AVAILABLE:
            try:
                # Base model - EfficientNetB0 pre-trained on ImageNet
                base_model = EfficientNetB0(
                    include_top=False,
                    input_shape=(IMG_SIZE, IMG_SIZE, 3),
                    weights="imagenet"
                )
                print("‚úÖ EfficientNetB0 loaded successfully")
                
                # Freeze the base model initially
                base_model.trainable = False
                
                # Add custom classification head
                x = base_model.output
                x = layers.GlobalAveragePooling2D(name="avg_pool")(x)
                x = layers.BatchNormalization()(x)
                x = layers.Dropout(0.3, name="top_dropout")(x)
                x = layers.Dense(256, activation="relu", name="dense_1")(x)
                x = layers.BatchNormalization()(x)
                x = layers.Dropout(0.2, name="dropout_2")(x)
                outputs = layers.Dense(NUM_CLASSES, activation="softmax", name="predictions")(x)
                
                # Create the model
                model = keras.Model(base_model.input, outputs, name="WeatherClassifier")
                
            except Exception as e:
                print(f"‚ö†Ô∏è  EfficientNetB0 loading failed: {e}")
                raise e
        else:
            raise Exception("EfficientNetB0 not available")
            
    except Exception as e:
        print(f"üîÑ Using fallback CNN architecture: {e}")
        
        # Fallback - Custom CNN architecture
        model = keras.Sequential([
            layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3)),
            
            # First block
            layers.Conv2D(32, (3, 3), activation='relu', padding='same'),
            layers.BatchNormalization(),
            layers.Conv2D(32, (3, 3), activation='relu', padding='same'),
            layers.MaxPooling2D(2, 2),
            layers.Dropout(0.25),
            
            # Second block
            layers.Conv2D(64, (3, 3), activation='relu', padding='same'),
            layers.BatchNormalization(),
            layers.Conv2D(64, (3, 3), activation='relu', padding='same'),
            layers.MaxPooling2D(2, 2),
            layers.Dropout(0.25),
            
            # Third block
            layers.Conv2D(128, (3, 3), activation='relu', padding='same'),
            layers.BatchNormalization(),
            layers.Conv2D(128, (3, 3), activation='relu', padding='same'),
            layers.MaxPooling2D(2, 2),
            layers.Dropout(0.25),
            
            # Fourth block
            layers.Conv2D(256, (3, 3), activation='relu', padding='same'),
            layers.BatchNormalization(),
            layers.Conv2D(256, (3, 3), activation='relu', padding='same'),
            layers.MaxPooling2D(2, 2),
            layers.Dropout(0.25),
            
            # Classification head
            layers.GlobalAveragePooling2D(),
            layers.BatchNormalization(),
            layers.Dropout(0.5),
            layers.Dense(512, activation='relu'),
            layers.BatchNormalization(),
            layers.Dropout(0.3),
            layers.Dense(256, activation='relu'),
            layers.Dropout(0.2),
            layers.Dense(NUM_CLASSES, activation='softmax')
        ], name="CustomCNN")
        
        print("‚úÖ Custom CNN model created successfully")
    
    # Compile the model
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=1e-3),
        loss="categorical_crossentropy",
        metrics=["accuracy"]
    )
    
    print("‚úÖ Model architecture built successfully")
    print(f"‚úÖ Total parameters: {model.count_params():,}")
    print(f"‚úÖ Trainable parameters: {sum([np.prod(v.shape) for v in model.trainable_weights]):,}")
    
    return model

def train_model_phase1(model, train_generator, validation_generator):
    """Phase 1: Train with frozen base model"""
    print(f"\nüéØ Phase 1: Training with frozen base model ({EPOCHS_INITIAL} epochs)")
    
    # Callbacks
    callbacks = [
        keras.callbacks.EarlyStopping(
            monitor='val_accuracy',
            patience=5,
            restore_best_weights=True,
            verbose=1
        ),
        keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=3,
            min_lr=1e-7,
            verbose=1
        ),
        keras.callbacks.ModelCheckpoint(
            'best_model_phase1.h5',
            monitor='val_accuracy',
            save_best_only=True,
            verbose=1
        )
    ]
    
    # Train the model
    history1 = model.fit(
        train_generator,
        epochs=EPOCHS_INITIAL,
        validation_data=validation_generator,
        callbacks=callbacks,
        verbose=1
    )
    
    return history1

def fine_tune_model(model, train_generator, validation_generator):
    """Phase 2: Fine-tune with unfrozen layers"""
    print(f"\nüéØ Phase 2: Fine-tuning model ({EPOCHS_FINE_TUNE} epochs)")
    
    try:
        # For Sequential models (our custom CNN), unfreeze the last few layers
        if hasattr(model, 'layers') and len(model.layers) > 0:
            # Check if this is a Sequential model
            if isinstance(model, keras.Sequential):
                print("üîß Fine-tuning Sequential model")
                # Unfreeze the last few layers for fine-tuning
                total_layers = len(model.layers)
                layers_to_unfreeze = min(5, total_layers // 2)  # Unfreeze last 5 layers or half
                
                # First freeze all layers
                for layer in model.layers:
                    layer.trainable = False
                
                # Then unfreeze the last few layers
                for layer in model.layers[-layers_to_unfreeze:]:
                    layer.trainable = True
                
                print(f"‚úÖ Unfroze last {layers_to_unfreeze} layers out of {total_layers}")
                
            else:
                # Functional model case (EfficientNet)
                print("üîß Fine-tuning Functional model")
                # Find the base model layer
                base_model = None
                for layer in model.layers:
                    if hasattr(layer, 'layers') and len(layer.layers) > 10:
                        base_model = layer
                        break
                
                if base_model:
                    base_model.trainable = True
                    # Fine-tune from the last 20 layers
                    fine_tune_at = max(0, len(base_model.layers) - 20)
                    for layer in base_model.layers[:fine_tune_at]:
                        layer.trainable = False
                    print(f"‚úÖ Fine-tuning from layer {fine_tune_at}")
                else:
                    # Fallback: unfreeze all
                    model.trainable = True
                    print("‚úÖ Unfreezing all layers")
        else:
            # Fallback: unfreeze all layers
            model.trainable = True
            print("‚úÖ Unfreezing all layers for fine-tuning")
        
        # Recompile with lower learning rate
        model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=1e-5),
            loss="categorical_crossentropy",
            metrics=["accuracy"]
        )
        
        print(f"‚úÖ Trainable parameters: {sum([np.prod(v.shape) for v in model.trainable_weights]):,}")
        
    except Exception as e:
        print(f"‚ö†Ô∏è  Fine-tuning setup failed: {e}")
        print("üîÑ Continuing with lower learning rate only...")
        
        # Just lower the learning rate
        model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=1e-5),
            loss="categorical_crossentropy",
            metrics=["accuracy"]
        )
    
    # Callbacks for fine-tuning
    callbacks = [
        keras.callbacks.EarlyStopping(
            monitor='val_accuracy',
            patience=5,
            restore_best_weights=True,
            verbose=1
        ),
        keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=3,
            min_lr=1e-7,
            verbose=1
        ),
        keras.callbacks.ModelCheckpoint(
            MODEL_SAVE_PATH,
            monitor='val_accuracy',
            save_best_only=True,
            verbose=1
        )
    ]
    
    # Continue training
    history2 = model.fit(
        train_generator,
        epochs=EPOCHS_FINE_TUNE,
        validation_data=validation_generator,
        callbacks=callbacks,
        verbose=1
    )
    
    return history2

def plot_training_history(history1, history2=None):
    """Plot training and validation metrics"""
    print("\nüìä Plotting training history...")
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # Combine histories if fine-tuning was performed
    if history2:
        acc = history1.history['accuracy'] + history2.history['accuracy']
        val_acc = history1.history['val_accuracy'] + history2.history['val_accuracy']
        loss = history1.history['loss'] + history2.history['loss']
        val_loss = history1.history['val_loss'] + history2.history['val_loss']
        epochs = range(1, len(acc) + 1)
        phase1_end = len(history1.history['accuracy'])
    else:
        acc = history1.history['accuracy']
        val_acc = history1.history['val_accuracy']
        loss = history1.history['loss']
        val_loss = history1.history['val_loss']
        epochs = range(1, len(acc) + 1)
        phase1_end = None
    
    # Accuracy plot
    axes[0, 0].plot(epochs, acc, 'b-', label='Training Accuracy')
    axes[0, 0].plot(epochs, val_acc, 'r-', label='Validation Accuracy')
    if phase1_end:
        axes[0, 0].axvline(x=phase1_end, color='g', linestyle='--', alpha=0.7, label='Fine-tuning starts')
    axes[0, 0].set_title('Model Accuracy')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Accuracy')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # Loss plot
    axes[0, 1].plot(epochs, loss, 'b-', label='Training Loss')
    axes[0, 1].plot(epochs, val_loss, 'r-', label='Validation Loss')
    if phase1_end:
        axes[0, 1].axvline(x=phase1_end, color='g', linestyle='--', alpha=0.7, label='Fine-tuning starts')
    axes[0, 1].set_title('Model Loss')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('Loss')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # Final metrics
    final_acc = acc[-1]
    final_val_acc = val_acc[-1]
    final_loss = loss[-1]
    final_val_loss = val_loss[-1]
    
    axes[1, 0].text(0.1, 0.8, f'Final Training Accuracy: {final_acc:.4f}', fontsize=12, transform=axes[1, 0].transAxes)
    axes[1, 0].text(0.1, 0.7, f'Final Validation Accuracy: {final_val_acc:.4f}', fontsize=12, transform=axes[1, 0].transAxes)
    axes[1, 0].text(0.1, 0.6, f'Final Training Loss: {final_loss:.4f}', fontsize=12, transform=axes[1, 0].transAxes)
    axes[1, 0].text(0.1, 0.5, f'Final Validation Loss: {final_val_loss:.4f}', fontsize=12, transform=axes[1, 0].transAxes)
    axes[1, 0].set_title('Final Metrics')
    axes[1, 0].axis('off')
    
    # Training summary
    axes[1, 1].text(0.1, 0.8, f'Total Epochs: {len(epochs)}', fontsize=12, transform=axes[1, 1].transAxes)
    axes[1, 1].text(0.1, 0.7, f'Best Validation Accuracy: {max(val_acc):.4f}', fontsize=12, transform=axes[1, 1].transAxes)
    axes[1, 1].text(0.1, 0.6, f'Image Size: {IMG_SIZE}x{IMG_SIZE}', fontsize=12, transform=axes[1, 1].transAxes)
    axes[1, 1].text(0.1, 0.5, f'Batch Size: {BATCH_SIZE}', fontsize=12, transform=axes[1, 1].transAxes)
    axes[1, 1].text(0.1, 0.4, f'Classes: {NUM_CLASSES}', fontsize=12, transform=axes[1, 1].transAxes)
    axes[1, 1].set_title('Training Configuration')
    axes[1, 1].axis('off')
    
    plt.tight_layout()
    plt.savefig('training_history.png', dpi=150, bbox_inches='tight')
    print("‚úÖ Training history saved as 'training_history.png'")
    plt.close()

def evaluate_model(model, test_generator):
    """Evaluate the trained model"""
    print("\nüìã Evaluating model performance...")
    
    if test_generator is None:
        print("‚ö†Ô∏è  No test data available for evaluation")
        return
    
    # Make predictions
    test_generator.reset()
    predictions = model.predict(test_generator, verbose=1)
    predicted_classes = np.argmax(predictions, axis=1)
    
    # Get true labels
    true_classes = test_generator.classes
    class_labels = list(test_generator.class_indices.keys())
    
    # Calculate accuracy
    test_accuracy = np.mean(predicted_classes == true_classes)
    print(f"‚úÖ Test Accuracy: {test_accuracy:.4f}")
    
    # Classification report
    print("\nüìä Classification Report:")
    print("=" * 50)
    print(classification_report(true_classes, predicted_classes, target_names=class_labels))
    
    # Confusion matrix
    cm = confusion_matrix(true_classes, predicted_classes)
    
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=class_labels, yticklabels=class_labels)
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.tight_layout()
    plt.savefig('confusion_matrix.png', dpi=150, bbox_inches='tight')
    print("‚úÖ Confusion matrix saved as 'confusion_matrix.png'")
    plt.close()

def convert_to_tflite(model_path):
    """Convert the trained model to TensorFlow Lite format"""
    print(f"\nüîÑ Converting model to TensorFlow Lite...")
    
    # Load the trained model
    model = keras.models.load_model(model_path)
    
    # Create TFLite converter
    converter = tf.lite.TFLiteConverter.from_keras_model(model)
    
    # Optimization settings
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    
    # Convert the model
    try:
        tflite_model = converter.convert()
        
        # Save the model
        os.makedirs(os.path.dirname(TFLITE_SAVE_PATH), exist_ok=True)
        with open(TFLITE_SAVE_PATH, 'wb') as f:
            f.write(tflite_model)
        
        print(f"‚úÖ TFLite model saved to: {TFLITE_SAVE_PATH}")
        print(f"‚úÖ Model size: {len(tflite_model) / (1024*1024):.2f} MB")
        
        # Test the converted model
        test_tflite_model(TFLITE_SAVE_PATH)
        
    except Exception as e:
        print(f"‚ùå Error during conversion: {e}")

def test_tflite_model(tflite_path):
    """Test the converted TFLite model"""
    print(f"\nüß™ Testing TFLite model...")
    
    try:
        # Load TFLite model
        interpreter = tf.lite.Interpreter(model_path=tflite_path)
        interpreter.allocate_tensors()
        
        # Get input and output details
        input_details = interpreter.get_input_details()
        output_details = interpreter.get_output_details()
        
        print(f"‚úÖ Input shape: {input_details[0]['shape']}")
        print(f"‚úÖ Output shape: {output_details[0]['shape']}")
        
        # Test with random input
        input_shape = input_details[0]['shape']
        input_data = np.random.random_sample(input_shape).astype(np.float32)
        
        interpreter.set_tensor(input_details[0]['index'], input_data)
        interpreter.invoke()
        
        output_data = interpreter.get_tensor(output_details[0]['index'])
        print(f"‚úÖ Test inference successful")
        print(f"‚úÖ Sample output: {output_data[0]}")
        
    except Exception as e:
        print(f"‚ùå Error testing TFLite model: {e}")

def main():
    """Main training pipeline"""
    print("üå§Ô∏è  Weather AI Model Training Pipeline")
    print("=" * 60)
    
    # Check data structure
    if not check_data_structure():
        print("\n‚ùå Please ensure your data is properly organized:")
        print("weather_images/")
        print("‚îú‚îÄ‚îÄ train_dir/")
        print("‚îÇ   ‚îú‚îÄ‚îÄ Cloudy/")
        print("‚îÇ   ‚îú‚îÄ‚îÄ Rain/")
        print("‚îÇ   ‚îú‚îÄ‚îÄ Sunrise/")
        print("‚îÇ   ‚îî‚îÄ‚îÄ Shine/")
        print("‚îî‚îÄ‚îÄ test_dir/")
        print("    ‚îú‚îÄ‚îÄ Cloudy/")
        print("    ‚îú‚îÄ‚îÄ Rain/")
        print("    ‚îú‚îÄ‚îÄ Sunrise/")
        print("    ‚îî‚îÄ‚îÄ Shine/")
        return
    
    # Load and preprocess data
    train_gen, val_gen, test_gen = load_and_preprocess_data()
    
    # Visualize samples
    visualize_data_samples(train_gen)
    
    # Build model
    model = build_weather_model()
    
    # Phase 1: Train with frozen base
    history1 = train_model_phase1(model, train_gen, val_gen)
    
    # Phase 2: Fine-tune
    history2 = fine_tune_model(model, train_gen, val_gen)
    
    # Plot training history
    plot_training_history(history1, history2)
    
    # Evaluate model
    if test_gen:
        evaluate_model(model, test_gen)
    
    # Convert to TFLite
    convert_to_tflite(MODEL_SAVE_PATH)
    
    print("\nüéâ Training pipeline completed!")
    print(f"‚úÖ Model saved as: {MODEL_SAVE_PATH}")
    print(f"‚úÖ TFLite model saved as: {TFLITE_SAVE_PATH}")
    print("‚úÖ Ready for integration with Flutter app!")

if __name__ == "__main__":
    main()
